#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
DeepSeek + Qdrant + Ollama åµŒå…¥æ¨¡å‹é…ç½®
ä½¿ç”¨ Ollama æœ¬åœ°è¿è¡ŒåµŒå…¥æ¨¡å‹ï¼Œå®Œå…¨ç¦»çº¿
"""

import os
import sys
from mem0 import Memory

# è®¾ç½® DeepSeek API Key
os.environ["DEEPSEEK_API_KEY"] = "sk-760dd8899ad54634802aafb839f8bda9"

# Ollama åµŒå…¥æ¨¡å‹é…ç½®
OLLAMA_CONFIG = {
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "collection_name": "deepseek_ollama_memories",
            "host": "localhost",
            "port": 6333,
            "embedding_model_dims": 768,  # nomic-embed-text çš„ç»´åº¦
            # å¦‚æœä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼Œè°ƒæ•´ç»´åº¦ï¼š
            # bge-m3: 1024
            # mxbai-embed-large: 1024
            # all-minilm: 384
            "on_disk": True,
        },
    },
    "llm": {
        "provider": "deepseek",
        "config": {
            "model": "deepseek-chat",
            "temperature": 0.7,
            "max_tokens": 3000,
            "top_p": 0.9,
        },
    },
    "embedder": {
        "provider": "ollama",
        "config": {
            "model": "nomic-embed-text:latest",  # Ollama åµŒå…¥æ¨¡å‹
            # å…¶ä»–å¯ç”¨çš„ Ollama åµŒå…¥æ¨¡å‹ï¼š
            # "model": "bge-m3:latest",  # BAAI ä¸­æ–‡åµŒå…¥æ¨¡å‹ï¼ˆæ¨èï¼‰
            # "model": "mxbai-embed-large:latest",  # å¦ä¸€ä¸ªå¥½çš„é€‰æ‹©
            # "model": "all-minilm:latest",  # è½»é‡çº§é€‰é¡¹
            "ollama_base_url": "http://localhost:11434",
        },
    },
}

# å„ç§ Ollama åµŒå…¥æ¨¡å‹çš„é…ç½®
OLLAMA_MODELS = {
    "nomic-embed-text": {
        "name": "nomic-embed-text:latest",
        "dims": 768,
        "description": "Nomic åµŒå…¥æ¨¡å‹ï¼Œå¤šè¯­è¨€æ”¯æŒ",
        "size": "274MB",
        "pull_command": "ollama pull nomic-embed-text"
    },
    "bge-m3": {
        "name": "bge-m3:latest",
        "dims": 1024,
        "description": "BAAI BGE-M3 ä¸­æ–‡åµŒå…¥æ¨¡å‹ï¼ˆæ¨èï¼‰",
        "size": "1.2GB",
        "pull_command": "ollama pull bge-m3"
    },
    "mxbai-embed-large": {
        "name": "mxbai-embed-large:latest",
        "dims": 1024,
        "description": "MixedBread AI åµŒå…¥æ¨¡å‹",
        "size": "670MB",
        "pull_command": "ollama pull mxbai-embed-large"
    },
    "all-minilm": {
        "name": "all-minilm:latest",
        "dims": 384,
        "description": "è½»é‡çº§åµŒå…¥æ¨¡å‹",
        "size": "45MB",
        "pull_command": "ollama pull all-minilm"
    },
    "snowflake-arctic-embed": {
        "name": "snowflake-arctic-embed:latest", 
        "dims": 1024,
        "description": "Snowflake åµŒå…¥æ¨¡å‹",
        "size": "669MB",
        "pull_command": "ollama pull snowflake-arctic-embed"
    }
}

def check_ollama_service():
    """æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ"""
    import requests
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=3)
        return response.status_code == 200
    except:
        return False

def list_ollama_models():
    """åˆ—å‡ºå·²å®‰è£…çš„ Ollama æ¨¡å‹"""
    import requests
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=3)
        if response.status_code == 200:
            models = response.json().get("models", [])
            return [m["name"] for m in models]
        return []
    except:
        return []

def download_embedding_model(model_name="nomic-embed-text"):
    """ä¸‹è½½ Ollama åµŒå…¥æ¨¡å‹"""
    model_info = OLLAMA_MODELS.get(model_name, OLLAMA_MODELS["nomic-embed-text"])
    
    print(f"ğŸ“¥ ä¸‹è½½ Ollama åµŒå…¥æ¨¡å‹: {model_info['name']}")
    print(f"   æè¿°: {model_info['description']}")
    print(f"   å¤§å°: {model_info['size']}")
    print(f"   ç»´åº¦: {model_info['dims']}")
    print(f"\n   æ‰§è¡Œå‘½ä»¤: {model_info['pull_command']}")
    
    import subprocess
    try:
        result = subprocess.run(
            ["ollama", "pull", model_info['name'].replace(":latest", "")],
            capture_output=True,
            text=True,
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
        )
        
        if result.returncode == 0:
            print(f"âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸï¼")
            return True
        else:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {result.stderr}")
            return False
    except subprocess.TimeoutExpired:
        print("â±ï¸ ä¸‹è½½è¶…æ—¶ï¼Œè¯·æ‰‹åŠ¨æ‰§è¡Œå‘½ä»¤")
        return False
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return False

def test_ollama_embedding():
    """æµ‹è¯• Ollama åµŒå…¥æ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯• Ollama åµŒå…¥æ¨¡å‹...")
    
    # æ£€æŸ¥ Ollama æœåŠ¡
    if not check_ollama_service():
        print("âŒ Ollama æœåŠ¡æœªè¿è¡Œ")
        print("   è¯·å…ˆå¯åŠ¨ Ollama: ollama serve")
        return False
    
    # æ£€æŸ¥å·²å®‰è£…çš„æ¨¡å‹
    installed_models = list_ollama_models()
    print(f"ğŸ“‹ å·²å®‰è£…çš„æ¨¡å‹: {installed_models}")
    
    # æ‰¾åˆ°å¯ç”¨çš„åµŒå…¥æ¨¡å‹
    available_embed_model = None
    for model in installed_models:
        for key, info in OLLAMA_MODELS.items():
            if info['name'] in model or key in model:
                available_embed_model = (key, info)
                break
        if available_embed_model:
            break
    
    if not available_embed_model:
        print("âŒ æœªæ‰¾åˆ°åµŒå…¥æ¨¡å‹ï¼Œå¼€å§‹ä¸‹è½½...")
        # ä¼˜å…ˆä¸‹è½½ä¸­æ–‡æ¨¡å‹
        if download_embedding_model("bge-m3"):
            available_embed_model = ("bge-m3", OLLAMA_MODELS["bge-m3"])
        elif download_embedding_model("nomic-embed-text"):
            available_embed_model = ("nomic-embed-text", OLLAMA_MODELS["nomic-embed-text"])
        else:
            print("âŒ æ— æ³•ä¸‹è½½åµŒå…¥æ¨¡å‹")
            return False
    
    # ä½¿ç”¨æ‰¾åˆ°çš„æ¨¡å‹é…ç½®
    model_key, model_info = available_embed_model
    print(f"\nâœ… ä½¿ç”¨åµŒå…¥æ¨¡å‹: {model_info['name']}")
    
    # æ›´æ–°é…ç½®
    config = OLLAMA_CONFIG.copy()
    config["embedder"]["config"]["model"] = model_info["name"]
    config["vector_store"]["config"]["embedding_model_dims"] = model_info["dims"]
    
    # æµ‹è¯•åˆå§‹åŒ–
    try:
        print("ğŸ”„ åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ...")
        m = Memory.from_config(config)
        
        # æµ‹è¯•æ·»åŠ è®°å¿†
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•è®°å¿†ï¼Œç”¨äºéªŒè¯ Ollama åµŒå…¥æ¨¡å‹"
        result = m.add(test_text, user_id="test_user")
        print(f"âœ… æ·»åŠ è®°å¿†æˆåŠŸ")
        
        # æµ‹è¯•æœç´¢
        results = m.search("æµ‹è¯•", user_id="test_user", limit=1)
        print(f"âœ… æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results['results'])} æ¡è®°å¿†")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def setup_ollama_embedding():
    """è®¾ç½® Ollama åµŒå…¥æ¨¡å‹çš„å®Œæ•´æµç¨‹"""
    print("ğŸš€ è®¾ç½® Ollama åµŒå…¥æ¨¡å‹")
    print("=" * 50)
    
    # 1. æ£€æŸ¥ Ollama æœåŠ¡
    if not check_ollama_service():
        print("\nâŒ Ollama æœåŠ¡æœªè¿è¡Œ")
        print("ğŸ”§ è¯·æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š")
        print("   1. å®‰è£… Ollama: brew install ollama")
        print("   2. å¯åŠ¨æœåŠ¡: ollama serve")
        return False
    
    print("âœ… Ollama æœåŠ¡æ­£å¸¸")
    
    # 2. åˆ—å‡ºå¯ç”¨æ¨¡å‹
    print("\nğŸ“‹ å¯ç”¨çš„åµŒå…¥æ¨¡å‹ï¼š")
    for key, info in OLLAMA_MODELS.items():
        print(f"   â€¢ {key}: {info['description']} ({info['size']})")
    
    # 3. æ¨èé…ç½®
    print("\nğŸ’¡ æ¨èé…ç½®ï¼š")
    print("   1. bge-m3 - ä¸­æ–‡è¯­ä¹‰ç†è§£æœ€ä½³")
    print("   2. nomic-embed-text - å¤šè¯­è¨€æ”¯æŒå¥½")
    print("   3. mxbai-embed-large - å¹³è¡¡é€‰æ‹©")
    
    # 4. æµ‹è¯•é…ç½®
    print("\n" + "=" * 50)
    return test_ollama_embedding()

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Ollama åµŒå…¥æ¨¡å‹é…ç½®')
    parser.add_argument('--download', '-d', 
                       choices=list(OLLAMA_MODELS.keys()),
                       help='ä¸‹è½½æŒ‡å®šçš„åµŒå…¥æ¨¡å‹')
    parser.add_argument('--test', '-t', action='store_true',
                       help='æµ‹è¯• Ollama åµŒå…¥é…ç½®')
    parser.add_argument('--setup', '-s', action='store_true',
                       help='å®Œæ•´è®¾ç½®æµç¨‹')
    
    args = parser.parse_args()
    
    if args.download:
        download_embedding_model(args.download)
    elif args.test:
        test_ollama_embedding()
    elif args.setup:
        setup_ollama_embedding()
    else:
        # é»˜è®¤æ‰§è¡Œè®¾ç½®æµç¨‹
        success = setup_ollama_embedding()
        
        if success:
            print("\nâœ… Ollama åµŒå…¥æ¨¡å‹é…ç½®æˆåŠŸï¼")
            print("ğŸ’¡ ç°åœ¨å¯ä»¥ä¿®æ”¹ deepseek_web_service.py ä½¿ç”¨ Ollama é…ç½®")
        else:
            print("\nâŒ é…ç½®å¤±è´¥ï¼Œè¯·æŒ‰æç¤ºæ“ä½œ")